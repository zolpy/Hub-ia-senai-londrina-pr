{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "O PCA é um dos métodos de aprendizado não-supervisionado mais utilizados para transformação de dados. É um algoritmo fundalmentalmente desenvolvido para redução de dimensionalidade, mas que também pode ser muito útil para visualização de dados, filtragem de ruido, extração de características e muito mais. Após uma discussão a respeito dos conceitos, nós veremos algumas aplicações desses conceitos.\n",
    "\n",
    "Começamos por importar os pacotes necessários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao PCA\n",
    "\n",
    "PCA é um método não supervisionado rápido e flexível para redução de dimensionalidade em dados. Seu comportamento é fácil de ser visualizado quando observamos um dataset com 2 dimensões. Considere, por exemplo, os 200 pontos a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Só de olhar fica claro que existe um relacionamento quase linear entre as variáveis $x$ e $y$. Apesar de lembrar o problema de regressão linear, em vez de tentar predizer os valores de $y$ com base em $x$, o aprendizado não supervisionado tenta aprender como funciona o relacionamento entre $x$ e $y$. \n",
    "\n",
    "No PCA, esse relacionamento é quantificado atravéz de uma lista dos eixos principais dos dados, os quais são empregados para descrever o conjunto de dados. Podemos computar o PCA da seguinte maneira usando o Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ajuste aprende algumas métricas dos dados, como os \"componentes\" e a \"variância explicada\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender o que esses números querem dizer, podemos visualizá-los como vetores sobre os dados, usando os \"componentes\" para definir a direção do vetor, e a \"variância explicada\" para definir o comprimento (ao quadrado) do vetor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, \n",
    "                            color='k')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plotar os dados\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "#plt.ylim(-1,1)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 1 * np.sqrt(length) # podemos alterar (didaticamente) para entender as componentes\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar os dados\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlim(-1.2,1.2)\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 1 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses vetores representam os eixos principais dos dados, e o comprimento do vetor indica o quão \"importante\" esse eixo é para descrever precisamente a distribuição dos dados. É uma medida de variância dos dados quando projetados nesses eixos. A projeção de cada ponto nos eixos principais são os \"componentes principais\" dos dados.\n",
    "\n",
    "Se plotarmos esses \"componentes principais\" ao lado dos dados originais, veremos os seguintes plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "pca = PCA(n_components=2, whiten=True) # whiten normalization, tomem nota!\n",
    "pca.fit(X)\n",
    "# CIFAR 10\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# plotando os dados\n",
    "ax[0].scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 1 * np.sqrt(length)\n",
    "    print(pca.mean_, v)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='x', ylabel='y', title='input')\n",
    "\n",
    "# plotando os componentes principais\n",
    "X_pca = pca.transform(X)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0,0], pca.components_[0], ax=ax[1])\n",
    "draw_vector([0,0], pca.components_[1], ax=ax[1])\n",
    "#for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    #v = vector * 1 *length#* np.sqrt(length)\n",
    "    #draw_vector([0,vector[0]], [0, vector[1]], ax=ax[1])\n",
    "#draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "#draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='component 1', ylabel='component 2',\n",
    "          title='principal components')#,xlim=(-5, 5), ylim=(-3, 3.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa transformação do eixo dos dados para os eixos principais é chamada de _affine_, a qual é basicamente composta por translação, rotação e escala uniforme.\n",
    "\n",
    "Embora esse algoritmo para encontrar os principais componentes possa parecer apenas uma curiosidade matemática, ele tem aplicações de longo alcance no mundo da aprendizagem de máquinas e exploração dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA para redução de dimensionalidade\n",
    "\n",
    "Usar o PCA para redução de dimensionalidade envolve zerar um ou mais dos menores componentes principais, resultando em uma projeção de menor dimensionalidade dos dados que preserve a variância máxima.\n",
    "\n",
    "Aqui temos um exemplo de como usar o PCA como uma transformação para redução de dimensionalidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados transformados foram reduzidos para uma única dimensão. Para entender o efeito dessa redução de dimensionalidade, podemos executar a transformada inversa dessa redução de dados e plotá-la com os dados originais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8, c='b')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pontos mais claros representam os dados originais, enquanto os pontos mais escuros representam a versão projetada (transformada). O plot deixa claro o que a redução de dimensionalidade do PCA significa: a informação distribuida pelos eixos menos importantes é removida, deixando apenas a componente dos dados com maior variância. A porção de variância que é removida é mais ou menos proporcional à medida de quanta \"informação\" é descartada nessa redução de dimensionalidade.\n",
    "\n",
    "Esse dataset com dimensão reduzida é de certa forma \"bom o suficiente\" para representar os relacionamentos mais importantes entre os pontos: apesar de reduzir as dimensões dos dados em 50%, a maior parte do relacionamento geral entre os dados é preservada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA para visualização\n",
    "\n",
    "A utilidade da redução de dimensionalidade pode não ser tão aparente em apenas 2 dimensões, mas se torna muito mais clara quando observamos dados com dimensionalidades muito mais altas. Como exemplo, podemos analisar uma aplicação de PCA para o dataset _Digits_.\n",
    "\n",
    "Vamos começar carregando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrem-se que os dados são compostos por imagens de 8x8 pixels, o que significa um conjunto de dados com 64 dimensões. Para termos uma ideia do relacionamento entre esses pontos, podemos usar o PCA para projetá-los em um espaço de dimensões mais fácil de \"manusear\", como por exemplo 2 dimensões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projetamos os dados de um espaço de 64 para 2 dimensões:\n",
    "\n",
    "pca = PCA(2)  \n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(\"Banco de dados original: \",digits.data.shape)\n",
    "print(\"Banco de dados transformado: \",projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos plotar os dois componentes principais de cada ponto e aprender sobre os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.xlabel('componente 1')\n",
    "plt.ylabel('componente 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando o que esses componentes significam...\n",
    "\n",
    "Os dados completos são uma \"nuvem de pontos\" de dimensão 64, e esses pontos são a projeção de cada ponto nas direções com maior variância. O que o PCA fez foi encontrar a rotação e o \"esticamento\" ótimo do espaço de 64 dimensões para uma projeção em 2 dimensões, de forma não supervisionada, sem referência aos rótulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que significam esses componentes?\n",
    "\n",
    "Podemos ir um pouco mais a fundo, e nos perguntarmos o que significa a redução de dimensões. O significado pode ser entendido em termos de combinação de vetores. Como exemplo, cada imagem no conjunto de treinamento é definida por uma coleção de 64 valores de pixels, as quais chamamos de vetor $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "x=[x_1,x_2,x_3,\\dots,x_{64}]\n",
    "\\end{equation}\n",
    "\n",
    "Imaginando o exemplo do pixel, poderiamos construir uma imagem multiplicando cada elemento do vetor pelo pixel que ele descreve, e depois somar o resultado para construir a imagem:\n",
    "\n",
    "\\begin{equation}\n",
    "image(x)=x_1\\cdot(pixel 1)+x_2\\cdot(pixel 2)+x_3\\cdot(pixel 3),\\dots,x_{64}\\cdot(pixel 64)\n",
    "\\end{equation}\n",
    "\n",
    "Um jeito que poderiamos imaginar de reduzir a dimensão seria zerar todas as bases do vetor exceto algumas. Por exemplo, se usássemos apenas os 8 primeiros pixels, teriamos uma projeção dos dados, mas que não reflete muito o conteúdo da imagem: jogariamos fora 90% dos pixels!\n",
    "\n",
    "<img src=\"assets/eightFirst.png\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linha de cima mostra os pixels individuais, e a de baixo mostra a distribuição cumulativa desses pixels para reconstruir a imagem. Usando apenas oito componentes de pixels, podemos reconstruir apenas uma pequena porção da imagem de 64 pixels. Para reconstruir toda a imagem com essa abordagem, precisariamos de todos os 64 pixels.\n",
    "\n",
    "Mas a abordagem de representação baseada em pixels não é a unica escolha possível. Podemos também considerar outras funções de base, as quais contém algumas contribuições pré-definidas de cada pixel, e podem ser escritas mais ou menos como da forma a seguir:\n",
    "\n",
    "\\begin{equation}\n",
    "image(x)=mean+x_1\\cdot(basis 1)+x_2\\cdot(basis 2)+x_3\\cdot(basis 3)\\dots\n",
    "\\end{equation}\n",
    "\n",
    "O PCA pode ser visualizado como um processo de escolha de funções de base ótimas, de modo que somando algumas das primeiras delas seja suficiente para reconstruir a estrutura dos elementos no dataset. Os componentes principais funcionam como uma representação dos dados em uma dimensionalidade menor, e são simplesmente os coeficientes que multiplicam cada um desses elementos nessa série. A figura abaixo mostra uma imagem similar a de cima, onde o dígito é reconstruído usando a média mais as 8 primeiras funções de base do PCA:\n",
    "\n",
    "<img src=\"assets/baseFunction.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferente da base em pixels, as funções de base do PCA permiter reconstruir as características mais salientes dos dados de entrada apenas com uma média mais oito componentes! A quantidade de pixels em cada componente é deduzida da orientação do vetor no exemplo em 2 dimensões. Esse é o sentido no qual o PCA proporciona uma representação de baixa dimensionalidade dos dados: ele descobre um conjunto de funções de base que são mais eficientes do que a representação em pixels dos dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolhendo o número de componentes\n",
    "\n",
    "Uma parte vital do PCA na prática é a habilidade de estimar quantos componentes são necessários para descrever os dados. Isso pode ser determinado olhando a razão da variância explicada cumulativa como uma função do número de componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('número de componentes')\n",
    "plt.ylabel('variância explicada cumulativa');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa curva quantifica quanto do total da variância das 64 dimensões estão contidas nos $N$ primeiros componentes. Como exemplo, podemos ver que os primeiros 10 componentes são responsáveis por aproximadamente 75% da variância, enquanto precisamos de aproximadamente 50 componentes para uma variância de 100%.\n",
    "\n",
    "Aqui podemos ver que nossa projeção bi-dimensional perde muita informação (medida pela variância explicada) e que precisamos de aproximadamente 20 componentes para manter 90% da variância. Olhar para o plot de um dataset de dimensão grande ajuda a compreender o **nível de redundância presente nos dados**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA para filtragem de ruídos\n",
    "\n",
    "O PCA também pode ser usado para filtrar ruídos nos dados. \n",
    "\n",
    "A ideia é a seguinte: Qualquer componente com variância muito maior que o efeito do ruído deve sofrer muito pouco com o efeito do ruído. Então se reconstruirmos os dados usando apenas o maior subconjunto dos componentes principais, estaremos dando preferência para o sinal e jogando fora o ruído!\n",
    "\n",
    "Vamos ver como funciona com o dataset _digits_. Primeiramente, iremos plotar diversas amostras sem ruído:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar um novo conjunto de dados adicionando ruído e plotando novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fica muito claro que as imagens são ruidosas, e contém pixels deformados. \n",
    "\n",
    "Vamos treinar o PCA sobre os dados ruidosos, mantendo uma projeção com 50% da variância:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.5).fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que 50% da variância representa 12 dos principais componentes. \n",
    "\n",
    "Agora que temos esses componentes, podemos usá-los para computar a transformada inversa e reconstruir os dígitos filtrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa propriedade de preservação do sinal/filtragem faz do PCA uma técnica muito útil de seleção de características. Como exemplo, em vez de treinar um classificador sobre um conjunto de dados de alta dimensionalidade, podemos treinar um classificador sobre uma representação de dimensionalidade bem menor, a qual pode servir automaticamente para filtrar o ruído aleatório nas amostras de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examplo: _Eigenfaces_ - Reconhecimento facial\n",
    "\n",
    "Na aula de SVM nós vimos como explorar o PCA para selecionar as características para tarefa de reconhecimento facial. Agora vamos explorar um pouco melhor o que acontece. Vamos usar o dataset Faces in the Wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada nos eixos principais que espalham o dataset. Por ser um dataset grande, nós usaremos o RandomizedPCA - que contém um método randomizado para aproximar os primeiros $N$ componentes principais de forma muito mais rápida que o PCA tradicional, e assim, é muito útil para dados de alta dimensionalidade (aqui, uma dimensionalidade de aproximadamente 3,000). Vamos dar uma olhada nos primeiros 150 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(150, svd_solver='randomized')\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, pode ser interessante visualizar as imagens associadas com os primeiros componentes principais (esses componentes são tecnicamente chamados de autovetores, então, esses tipos de imagens são, com frequencia, chamados \"eigenfaces\"). Como podemos ver na figura abaixo, essas imagens são um tanto quanto estranhas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados são bem interessantes, e nos dão algumas ideias sobre como essas imagens variam. Por exemplo, as primeiras _eigenfaces_ (no topo à esquerda) parecem estar associadas com o ângulo de iluminação da face, enquanto  os próximos vetores principais parecem extrair algumas características específicas, como os olhos, narizes, e lábios. \n",
    "\n",
    "Vamos dar uma olhada na variância cumulativa desses componentes e ver quanta informação a projeção pode preservar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate((np.repeat(80,10).reshape(-1,1), np.arange(0,1,0.1).reshape(-1,1)),axis=1)#.reshape((10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.plot(0.9*np.ones(150), color='red')\n",
    "plt.plot(a[:,0], a[:,1], '--', color='red')\n",
    "plt.plot(0.95*np.ones(150), color='green')\n",
    "plt.xlabel('Número de componentes')\n",
    "plt.ylabel('Variância explicada acumulada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que os 150 componentes são capaz de representar mais de 90% da variância. Isso nos leva a acreditar que usando esses 150 podemos recuperar a maior parte das características essenciais dos dados. Para deixar mais concreto, podemos comparar a imagem de entrada com as imagens reconstruidas a partir desses 150 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computa os componentes mais importantes e projeta as faces\n",
    "pca = PCA(150, svd_solver='randomized').fit(faces.data)\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota os resultados\n",
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "#plt.figure(figsize=(10, 8), dpi=300)    \n",
    "ax[0, 0].set_ylabel('full-dim\\nentrada')\n",
    "ax[1, 0].set_ylabel('150-dim\\nreconstrução');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linha de cima mostra as imagens de entrada, enquanto as imagens de baixo mostram a reconstrução das imagens  usando apenas 150 das aproximadamente 3,000 características iniciais. Essa visualização deixa claro o porque a seleção de características usando PCA é bem sucedida: embora reduza a dimensionalidade em aproximadamente 20 vezes, as imagens projetadas contêm informação suficiente para que possamos, só de olhar, reconhecer os indivíduos nas imagens. O que significa é que nosso algoritmo de classificação precisa ser treinado sobre um conjunto de dados com 150 dimensões em vez de 3,000. Dependendo do algoritmo de classificação, essa mudança pode levar a uma classificação muito mais eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Nesta aula nós discutimos o uso do PCA para a redução de dimensionalidade, visualização de dados de alta dimensionalidade, filtragem de ruídos e para seleção de características de dados de alta dimensionalidade. Por causa da versatilidade e interpretabilidade do PCA, foi mostrado que a técnica é efetiva em uma grande variedade de contextos e problemas. \n",
    "\n",
    "Dado qualquer conjunto de dados de alta dimensionalidade, o PCA permite visualizar o relacionamento entre os pontos (como fizemos com os dígitos), entender a variância dos dados (como fizemos com as _eigenfaces_ ), e entender a dimensionalidade intrínsica (plotando a razão da variância explicada). Com certeza, o PCA pode não ser útil para todos os datasets com alta dimensionalidade, mas ele oferece uma maneira simples e eficiente de obter _insights_ sobre esses dados.\n",
    "\n",
    "A maior fraqueza do PCA é que ele tende a ser altamente impactado por anomalias nos dados. Sendo assim, algumas variantes do PCA foram desenvolvidas para tentar mitigar esse problema, agindo geralmente para descartar de modo iterativo os pontos que são descritos de maneira mais pobre pelos componentes iniciais. \n",
    "\n",
    "O Scikit-learn contém algumas dessas variantes, como o `RandomizedPCA`, que usa um método não-determinístico para aproximar rápidamente os principais componentes em dados com dimensão muito altas, e o `SparsePCA`, que introduz um termo de regularização que serve para forçar a esparsidade dos componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
