{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência e validação\n",
    "\n",
    "Agora que treinamos a rede, podemos usá-la para fazer predições. Esse processo é chamado de **inferência**, um termo emprestado da estatística. No entanto, redes neurais tem uma tendencia de aprender *muito bem* o comportamento dos dados de treinamento, e as vezes apresentam dificuldade para generalizar esse conhecimento para dados desconhecidos, i.e., conjunto de testes. Esse problema é chamado de **overfitting**, o qual prejudica a performance da inferência. Para testar se está ocorrendo overfitting enquanto treinamos o modelo, podemos avaliar a sua performance em um conjunto especial de dados chamado conjunto de **validação**. Podemos evitar o _overfitting_ usando técnicas de regularização, como por exemplo **dropout**, enquanto monitoramos a performance da validação durante o treinamento.\n",
    "\n",
    "Como de costume, vamos começar carregando o conjunto de dados utilizando torchvision. Podemos baixar o conjunto de testes setando `train=False`:\n",
    "\n",
    "```python\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "```\n",
    "\n",
    "O conjunto de teste geralmente contém 20% a 30% do tamanho total do dataset, e é usado para validar e testar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f824916c4f9456a8cf188e64ea5d74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0f548dd2ff429fa9afe0b211352c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b9b35069334e31b8feadad68077b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92341e869c924058828e62da7c230eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/roder/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roder/anaconda3/envs/deep/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define as transformações para normalizar os dados\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Baixa e carrega o conjunto de treinamento\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# baixa e carrega o conjunto de testes\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um model MLP com 3 camadas escondidas, uma com 256, uma com 128 e uma com 64 neurônios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # certificar que o tensor com as imagens foi 'achatado'\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do conjunto de validação é medir a performance do modelo durante o treinamento sem usar as amostras empregadas para ajustar os pesos. Essa performance pode ser medida pela acurácia, taxa de acerto, entre outras opções. No exemplo usaremos a acurácia. Vamos começar fazendo o passo _forward_ sobre o conjunto de testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "# Computa a probabilidade das classes (lembrando que usa exp por que a saída da rede é log_softmax)\n",
    "ps = torch.exp(model(images))\n",
    "# confirmando se o formato da saída é apropriada, i.e., 64 amostras com probabilidade de pertencer a 10 classes\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com esse vetor de probabilidades, podemos pegar a classe mais provável usando o método `ps.topk`, o qual retorna os $k$ maiores valores. Como queremos saber qual é a (única) classe mais provavel, usamos `ps.topk(1)`. A função retorna uma tupla com os $k$ maiores valores e seus indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Veja as classes mais prováveis para as 10 primeiras amostras\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar se a classe estimada corresponde ao rótulo, comparamos `top_class` e `labels`. Lembre que `top_class` é um tensor 2D com formato `(64, 1)`, enquanto `labels` é 1D com formato `(64)`, e para igualá-los, precisamos ajustar o formato do nosso tensor labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_class.shape =  torch.Size([64, 1])\n",
      "*top_class.shape =  64 1\n",
      "equals.shape =  torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "equals = top_class == labels.view(*top_class.shape)\n",
    "# * quer dizer que está desempacotando os valores da lista top_class.shape, ou seja, é o mesmo que \n",
    "#   fazer labels.view(top_class.shape[0],top_class.shape[1])\n",
    "\n",
    "print('top_class.shape = ', top_class.shape)\n",
    "print('*top_class.shape = ', *top_class.shape)\n",
    "print('equals.shape = ', equals.shape)\n",
    "#print('equals = ', equals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora precisamos calcular a porcentagem de predições corretas. `equals` é composto por uma lista de valores binarios, o que significa que se somarmos os acertos e dividir pelo tamanho do vetor teremos a porcentagem de estimativas corretas, ou seja, é o mesmo que computar a média, o que poderia ser feito usando `torch.mean`. No entanto, usando `torch.mean(equals)` teremos o erro:\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor \n",
    "```\n",
    "ou\n",
    "```\n",
    "RuntimeError: Can only calculate the mean of floating types. Got Bool instead.\n",
    "```\n",
    "\n",
    "Isso porque `equals` é do tipo `torch.ByteTensor` ou `torch.bool`, mas `torch.mean` não funciona pra esses tipos de dados. Então devemos converter `equals` para tensor de floats. Note que `torch.mean` retorna um tensor com um escalar, e para pegar o valor de fato desse tensor devemos usar `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 7.8125%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Acurácia: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rede não foi treinada, por isso apresentou resultados ruins. Agora vamos treinar a rede e incluir um passo de validação para avaliar o quanto a rede progride sobre dados desconhecidos. Visto que não vamos computar os gradientes nessa etapa, devemos desligar o autograd para deixar o processo mais rápido usando `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "# desligando o calculo dos gradientes\n",
    "with torch.no_grad():\n",
    "    # passo de validação\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: 1/30..  Training Loss: 0.514..  Test Loss: 0.460..  Test Accuracy: 0.833\n",
      "Epoch: 2/30..  Training Loss: 0.397..  Test Loss: 0.433..  Test Accuracy: 0.848\n",
      "Epoch: 3/30..  Training Loss: 0.358..  Test Loss: 0.441..  Test Accuracy: 0.843\n",
      "Epoch: 4/30..  Training Loss: 0.338..  Test Loss: 0.375..  Test Accuracy: 0.868\n",
      "Epoch: 5/30..  Training Loss: 0.316..  Test Loss: 0.374..  Test Accuracy: 0.866\n",
      "Epoch: 6/30..  Training Loss: 0.303..  Test Loss: 0.368..  Test Accuracy: 0.872\n",
      "Epoch: 7/30..  Training Loss: 0.296..  Test Loss: 0.379..  Test Accuracy: 0.873\n",
      "Epoch: 8/30..  Training Loss: 0.282..  Test Loss: 0.368..  Test Accuracy: 0.872\n",
      "Epoch: 9/30..  Training Loss: 0.272..  Test Loss: 0.377..  Test Accuracy: 0.871\n",
      "Epoch: 10/30..  Training Loss: 0.267..  Test Loss: 0.382..  Test Accuracy: 0.874\n",
      "Epoch: 11/30..  Training Loss: 0.259..  Test Loss: 0.378..  Test Accuracy: 0.878\n",
      "Epoch: 12/30..  Training Loss: 0.254..  Test Loss: 0.378..  Test Accuracy: 0.877\n",
      "Epoch: 13/30..  Training Loss: 0.251..  Test Loss: 0.379..  Test Accuracy: 0.872\n",
      "Epoch: 14/30..  Training Loss: 0.246..  Test Loss: 0.391..  Test Accuracy: 0.873\n",
      "Epoch: 15/30..  Training Loss: 0.238..  Test Loss: 0.399..  Test Accuracy: 0.877\n",
      "Epoch: 16/30..  Training Loss: 0.235..  Test Loss: 0.386..  Test Accuracy: 0.879\n",
      "Epoch: 17/30..  Training Loss: 0.231..  Test Loss: 0.378..  Test Accuracy: 0.880\n",
      "Epoch: 18/30..  Training Loss: 0.227..  Test Loss: 0.379..  Test Accuracy: 0.886\n",
      "Epoch: 19/30..  Training Loss: 0.223..  Test Loss: 0.420..  Test Accuracy: 0.873\n",
      "Epoch: 20/30..  Training Loss: 0.222..  Test Loss: 0.396..  Test Accuracy: 0.876\n",
      "Epoch: 21/30..  Training Loss: 0.221..  Test Loss: 0.388..  Test Accuracy: 0.884\n",
      "Epoch: 22/30..  Training Loss: 0.212..  Test Loss: 0.422..  Test Accuracy: 0.880\n",
      "Epoch: 23/30..  Training Loss: 0.209..  Test Loss: 0.505..  Test Accuracy: 0.861\n",
      "Epoch: 24/30..  Training Loss: 0.205..  Test Loss: 0.406..  Test Accuracy: 0.882\n",
      "Epoch: 25/30..  Training Loss: 0.207..  Test Loss: 0.401..  Test Accuracy: 0.879\n",
      "Epoch: 26/30..  Training Loss: 0.204..  Test Loss: 0.406..  Test Accuracy: 0.886\n",
      "Epoch: 27/30..  Training Loss: 0.199..  Test Loss: 0.430..  Test Accuracy: 0.880\n",
      "Epoch: 28/30..  Training Loss: 0.192..  Test Loss: 0.397..  Test Accuracy: 0.887\n",
      "Epoch: 29/30..  Training Loss: 0.192..  Test Loss: 0.445..  Test Accuracy: 0.885\n",
      "Epoch: 30/30..  Training Loss: 0.191..  Test Loss: 0.426..  Test Accuracy: 0.883\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        # else pode ser usado em laços tipo for no python, e só será executado se não cair em nenhum break\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            # passo de validação\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                ps = torch.exp(log_ps)\n",
    "                \n",
    "                test_loss += criterion(log_ps, labels)  \n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy+= torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss / len(trainloader))\n",
    "        test_losses.append(test_loss / len(testloader))\n",
    "                \n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Se olharmos para os valores de loss sobre o conjunto de treinamento e de teste veremos um fenômeno conhecido como overfitting:\n",
    "\n",
    "<img src='assets/overfitting.png' width=450px>\n",
    "\n",
    "A rede aprende o comportamento do conjunto de treinamento cada vez melhor, reduzindo o erro nesse contexto. No entanto, tem problemas para generalizar e classificar corretamente os dados desconhecidos. O objetivo maior dos algoritmos de aprendizado de máquina é poder predizer e fazer estimativas sobre dados desconhecidos, por isso o objetivo deve ser minimizar o erro sobre o conjunto de validação/testes. Uma possível solução, chamada *parada-antecipada* (_early-stop_) seria salvar o modelo a cada época e usar a rede com melhor resultado sobre o conjunto de validação, no entanto não é a melhor opção.\n",
    "\n",
    "A abordagem mais comum para reduzir overfitting é a utilização de técnicas de regularização. Veremos a seguir como usar uma técnica chamada *dropout* para essa tarefa, a qual seleciona de forma aleatória, a cada iteração, alguns neurônios para serem desligados. Dropout força a rede a compartilhar informações pelos pesos, reforçando seu poder de generalização. Adicionar dropout ou PyTorch é bem simples, basta utilizar o módulo [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout).\n",
    "\n",
    "\n",
    "<img src='assets/dropout.png' width=550px>\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Módulo Dropout com probabilidade de 0.2 para desligar um neurônio\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # aplicando 'flattening' nas imagens\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Agora com dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # saída - não há dropout aqui\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "Durante o treinamento nos queremos usar dropout para evitar overfitting, mas durante a inferência, queremos usar a rede completa. Sendo assim, precisamos desligar o dropout durante o processo de validação, teste, ou qualquer etapa em que usemos a rede para fazer predições. Para isso, usaremos `model.eval()`, que seta o modelo para o modo de avaliação e desliga o dropout. O dropout pode ser religado voltando ao modo de treino com `model.train()`. No geral, o padrão para o laço de validação tem o formato a seguir, onde a desligamos a computação do gradiente e setamos o modelo para modo de avaliação, calculamos o loss e as métricas, e depois setamos de volta para o modo de treinamento.\n",
    "\n",
    "```python\n",
    "# desligar a computação dos gradientes\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # setar para modo de avaliação\n",
    "    model.eval()\n",
    "    \n",
    "    # passagem das amostras de validação/teste\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# volta o modelo para modo treinamento\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        x = F.log_softmax(self.fc4(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roder/anaconda3/envs/deep/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 0.603..  Test Loss: 0.460..  Test Accuracy: 0.834\n",
      "Epoch: 2/30..  Training Loss: 0.487..  Test Loss: 0.486..  Test Accuracy: 0.835\n",
      "Epoch: 3/30..  Training Loss: 0.451..  Test Loss: 0.440..  Test Accuracy: 0.838\n",
      "Epoch: 4/30..  Training Loss: 0.433..  Test Loss: 0.404..  Test Accuracy: 0.855\n",
      "Epoch: 5/30..  Training Loss: 0.418..  Test Loss: 0.392..  Test Accuracy: 0.861\n",
      "Epoch: 6/30..  Training Loss: 0.415..  Test Loss: 0.423..  Test Accuracy: 0.846\n",
      "Epoch: 7/30..  Training Loss: 0.406..  Test Loss: 0.401..  Test Accuracy: 0.854\n",
      "Epoch: 8/30..  Training Loss: 0.397..  Test Loss: 0.419..  Test Accuracy: 0.852\n",
      "Epoch: 9/30..  Training Loss: 0.396..  Test Loss: 0.383..  Test Accuracy: 0.861\n",
      "Epoch: 10/30..  Training Loss: 0.385..  Test Loss: 0.390..  Test Accuracy: 0.860\n",
      "Epoch: 11/30..  Training Loss: 0.380..  Test Loss: 0.392..  Test Accuracy: 0.859\n",
      "Epoch: 12/30..  Training Loss: 0.387..  Test Loss: 0.377..  Test Accuracy: 0.869\n",
      "Epoch: 13/30..  Training Loss: 0.381..  Test Loss: 0.395..  Test Accuracy: 0.862\n",
      "Epoch: 14/30..  Training Loss: 0.379..  Test Loss: 0.374..  Test Accuracy: 0.869\n",
      "Epoch: 15/30..  Training Loss: 0.365..  Test Loss: 0.374..  Test Accuracy: 0.873\n",
      "Epoch: 16/30..  Training Loss: 0.372..  Test Loss: 0.381..  Test Accuracy: 0.869\n",
      "Epoch: 17/30..  Training Loss: 0.366..  Test Loss: 0.388..  Test Accuracy: 0.867\n",
      "Epoch: 18/30..  Training Loss: 0.364..  Test Loss: 0.387..  Test Accuracy: 0.874\n",
      "Epoch: 19/30..  Training Loss: 0.355..  Test Loss: 0.368..  Test Accuracy: 0.872\n",
      "Epoch: 20/30..  Training Loss: 0.358..  Test Loss: 0.389..  Test Accuracy: 0.866\n",
      "Epoch: 21/30..  Training Loss: 0.359..  Test Loss: 0.375..  Test Accuracy: 0.870\n",
      "Epoch: 22/30..  Training Loss: 0.355..  Test Loss: 0.372..  Test Accuracy: 0.873\n",
      "Epoch: 23/30..  Training Loss: 0.350..  Test Loss: 0.371..  Test Accuracy: 0.876\n",
      "Epoch: 24/30..  Training Loss: 0.353..  Test Loss: 0.365..  Test Accuracy: 0.875\n",
      "Epoch: 25/30..  Training Loss: 0.343..  Test Loss: 0.392..  Test Accuracy: 0.867\n",
      "Epoch: 26/30..  Training Loss: 0.341..  Test Loss: 0.367..  Test Accuracy: 0.875\n",
      "Epoch: 27/30..  Training Loss: 0.349..  Test Loss: 0.368..  Test Accuracy: 0.875\n",
      "Epoch: 28/30..  Training Loss: 0.337..  Test Loss: 0.370..  Test Accuracy: 0.873\n",
      "Epoch: 29/30..  Training Loss: 0.352..  Test Loss: 0.381..  Test Accuracy: 0.870\n",
      "Epoch: 30/30..  Training Loss: 0.339..  Test Loss: 0.377..  Test Accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for image, label in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(image)\n",
    "        loss = criterion(log_ps, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss.item()\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for image, label in testloader:         \n",
    "                \n",
    "                log_ps = model.forward(image)\n",
    "                prob = torch.exp(log_ps)\n",
    "                \n",
    "                test_loss+= criterion(log_ps, label)\n",
    "                \n",
    "                \n",
    "                k_prob, k_class = prob.topk(1, dim=1)\n",
    "                equals = k_class == label.view(*k_class.shape)\n",
    "                \n",
    "                accuracy+= torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        model.train()\n",
    "    train_losses.append(running_loss/len(trainloader))\n",
    "    test_losses.append(test_loss/len(testloader))\n",
    "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "          \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferência\n",
    "\n",
    "Agora que treinamos o modelo, podemos usá-lo para inferência. Já fizemos isso antes, mas agora precisamos lembrar de setar o modelo para o mode de inferência com `model.eval()`. Precisamos também desligar o autograd usando `torch.no_grad()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roder/anaconda3/envs/deep/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADZCAYAAAB1u6QQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAd/UlEQVR4nO3deXxdZb3v8c83LS2UYltoy1wCiCiDUgyKA7YMZVQQBWVSq8dbRZFzRL2i1xnuOSii1yNOXEWkMhQ4yqBgqZQWuJQhRZBBkFrLWDDQUjq3aX7nj/Xkst2slSbpzt4ryff9euWVvX9revZK4ZfnWU+enyICMzOzsmlqdAPMzMzyOEGZmVkpOUGZmVkpOUGZmVkpOUGZmVkpOUGZmVkpOUGZWcNJ+oakXze6HT0lqVlSSBray+ND0msLtp0q6ea8fSX9VNJXe9fq/sMJyszqQtIpklolrZC0WNJNkt7ZoLaEpJWpLc9I+p6kIY1oS5GIuCwiDi/Y9smIOAdA0mRJT9e3dfXhBGVmfU7SWcD/Af4d2BaYAPwYOK6BzXpTRIwEDgVOAf5H9Q697RlZbThBmVmfkjQK+Bbw6Yj4TUSsjIj1EXFDRHyh4JirJT0naZmk2yTtXbHtaEmPSFqeej+fT/Gxkn4n6SVJSyTdLmmj/4+LiEeB24F9Kobs/kXSk8BsSU2SviLpCUn/kHRp+kyVPibp2dQz/HxFW98iaV5q02JJF0oaVnXs0ZIWSnpB0vmdbZY0VdIdBffnEknnStoSuAnYIfUGV0jaQdIqSdtU7L+/pDZJm23sfpSJE5SZ9bW3AZsDv+3BMTcBewDjgfuAyyq2/QL4RERsBewDzE7xzwFPA+PIemlfBja6lpukvYCDgD9VhCcBbwCOAKamr4OB3YCRwIVVpzk4tfdw4IuSDkvxDcBngbFk9+FQ4FNVxx4PtAD7k/UoP7axNneKiJXAUcCzETEyfT0LzAE+ULHrh4ArI2J9d89dBk5QZtbXtgFeiIj27h4QERdHxPKIWAt8A3hTRa9lPbCXpNdExNKIuK8ivj2wS+qh3R5dLzZ6n6SlwA3Az4FfVmz7RurprQZOBb4XEQsjYgXwJeCkquG/b6b9H0znOTl9jvkRcVdEtEfEIuBnZMmv0rcjYklEPEk2DHpyd+9TF34FnAaQnq2dDEyvwXnrygnKzPrai8DY7j7PkTRE0nmS/ibpZWBR2jQ2fX8/cDTwhKS5kt6W4ucDC4Cb05DZ2Ru51P4RMSYido+Ir0RER8W2pype7wA8UfH+CWAoWS8tb/8n0jFIel0adnwufZZ/r/gcXR67ia4jS+K7AlOAZRFxTw3OW1dOUGbW1+YBa4H3dnP/U8iGug4DRgHNKS6AiLg3Io4jG/67FrgqxZdHxOciYjfgWOAsSYf2ss2VPa9ngV0q3k8A2oHnK2I7V21/Nr3+CfAosEdEvIZs2FFV1yo6tjdtzQIRa8juy2lkw3v9rvcETlBm1sciYhnwNeBHkt4raYSkzSQdJek7OYdsRZbQXgRGkPU6AJA0LP190Kj0POVloCNte7ek10oSsIzs+U/Hq87ec1cAn5W0q6SRqT0zqoYsv5o+197AR4EZFZ/lZWCFpNcDp+ec/wuSxkjaGfjXimO763lgm5yJG5eSPTs7FicoM7N8EXEBcBbwFaCNbFjrDLIeULVLyYa6ngEeAe6q2v4hYFEaMvsk2TMiyCYp/BFYQdZr+3FE3FqD5l9M9j/424C/A2uAz1TtM5dsePEW4LsR0fkHtp8n6xEuB/4v+cnnOmA+cD/we7JJIN2WZiFeASxMswV3SPH/R5ag74uIJ7o6R1nJBQvNzAYmSbOByyPi541uS284QZmZDUCSDgBmATtHxPJGt6c3PMRnZjbASPoV2XDnv/XX5ATuQZmZWUl1+XcJU5pOdPayfmdWx9XV03jNrB/yEJ+ZmZWSV+o1G0DGjh0bzc3NjW6GWY/Mnz//hYgYVx13gjIbQJqbm2ltbW10M8x6RFLu32l5iM/MzErJCcrMzErJCcrMzErJCcrMzErJCcrMzErJCcrMzErJCcrMzErJCcrMzErJCcrMzErJCcoGPUl3SvraRvZplnRNVWyypO928xqPS5qTrvW9XrRxWk+PMevvnKBsUJO0M/A0MLmPL7UsIiZHxNuBfSTt1MPjnaBs0HGCssHuBOAy4FFJrweQ9A1J0yXdKGmupC06d5bUJOknkj5ceRJJR0q6PfWQTi66mKQmYBiwOr2/QNIdkmZLak6xsyTNS/H9JR0P7Jl6YKfU+gaYlZUTlA12hwN/AK4ATqyIPx4RRwN3AVNSbAjwc2BORFzauaMkAV8FDgUOAs6QNKTqOqMkzQEeAp6LiBcltQA7RsQ7ga8DX5O0HfBe4B3AacC3I+K3wGOpB3Z59QeQNE1Sq6TWtra2TbkXZqXiBGWDVhpm2we4DvgKcEzF5j+l708BY9LrtwLbRcSMqlONA14H3AzcAoxOsUqdQ3x7Ac9KOgl4LXBv2n4vsAfQDDwQER0RsSidq0sRcVFEtEREy7hxr6pYYNZvOUHZYHYC8NmIODIijgDuk7Rn2lZZTbqzQu+dwExJF1Sd5wXgUeDwiJgM7BcRz3Vx3aXAeGABcECKHQA8DiwC9ktDic3ASzntMRsUXA/KBrP3kw2ndboV+EBXB0TEDyR9WdK3gNkp1iHpXGCWpA6gLec8nUN8AtYAH4yIlyQtlnQH0A58NCKek3QdWTLsAD7T2bYU/2VEXNvrT2zWjyii+BezKU0n+rc263dmdVytje81MLW0tIQLFlp/I2l+RLRUxz3EZ2ZmpeQEZWZmpeQEZWZmpeQEZWZmpeQEZWZmpeQEZWZmpeQEZWZmpeQEZWZmpeQEZdYAqb5UW1qh/N60Np+ZVXCCMmucuWntvoOALzS4LWal47X4zBpvBLBK0r7AhWT1ouZHxBmShgJXkq1q/hiwZURMbVRDzerJPSizxpmUFpB9ELicbHXzyRHxNmBnSXuQLWb714g4DHgg7ySuB2UDlROUWeN0DvE1A6eSldy4UdJcYH9gB7KaUfPT/vNzzuF6UDZgOUGZNVhErAfWAt8ELoiISWQFE0XWq5qYdp2YfwazgcnPoMwap3OIb3PgHuB3wA8kPcorvzxeC5wk6RZgIbC+Ae00awgnKLMGSOXc88bj9q4OSDo5ItZLmsYr5efNBjwnKLPyu07SSLJhwA82ujFm9eIEZVZyEXF0o9tg1gieJGFmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGXWTZK2knRDKpExT9JRm3i+yZK+W6v2mQ00nmZu1n0fBv4QET+SJGBUvRsgqSkiOup9XbNGcA/KrPtWAwdK2jYyL0n6i6RfSbpf0qkAknaTNDP1tL6fYvtKmpt6XhdWnlTS5pKukjSl4Nipkq6UdANwZL0/tFmjOEGZdd90sppMM1Oi2RPYDvgM8C7gzLTfecCn0krlm0tqIb+UBmS1oK4AfhQRswqOBVgfEe+JiBurG+VyGzZQeYjPrJvSquPnAudKmkK2+vjCiHgZQNKQtOvrgV9ko4BsBcwEVgEXSBoB7EZWSgPgOOD6iJjbxbEA93bRrouAiwBaWlpi0z+pWTm4B2XWTZJ2kTQsvf0HWTmMvITwGPCR1AtqIVul/HReXUoDst7TGklndnEsgJ872aDjHlSD/PUXLbnx0fcNy42P/9Gdfdmc/kna+D61tS8wQ9IasgTzaeCSnP2+CPxU0ubABuBjwA28upQGABHxWUk/lfSxgmPNBiUnKLNuiojf8UqPplNLxfaW9H0hUD0F/UlySmkAc9Ixn6yIVR97Sc9ba9b/eYjPzMxKyQnKzMxKyQnKzMxKyQnKzMxKaXBPkiiaBRa1+1OSodttm79hff7vBh35k/jQxLzn65n408M9bVafG9o8ITfevujJ2l2khj8nMysf96DMzKyUnKDMzKyUnKDMzKyUnKDM6iCvlpSk1pz9zpa0a058asUyS2aDwuCeJGFWP92qJRUR51XHJDUBU4FrgHV92UizMhkcCapGs/WaRowo3Nb+5j1z4xtWr8+Nv+H7S3LjTx4/Pje+eHJxbbzN9n9bbnz8vBdz4/HU4tx4x/LlufGiGXkAf/3Ujrnx4Uvy7/n2U/Lv+fKL888DEE355xo9fV7hMSW0GjhY0jUR8TzwkqQtJf0KeBNwfkRcJukS4LvAWOBzQDvQCuwH3CTptxHxvYZ8ArM6GxwJyqzxppOV2JgpaTVZj6izlhTALOCyqmNGAZMiIlJ5j3dHxIrqE0uaBkwDmDCh+JcJs/7Gz6DM6iAi1kfEuRGxH/A1KmpJpXpSQ3IOa43YeDc/Ii6KiJaIaBk3blxtG27WQE5QZnXQg1pSlSprQK0nP4mZDVhOUGb1sS9wm6Q5wH8C5/Tw+OuBq9Jwntmg4GdQZnXQg1pSUyu2z6nY/kPgh33XQrPyaXiC0tCeNSHa23t+kYJh/CGj82fGLZvy+tx4x9DiCq5jHsiflbfhL4/nxwvatON5C3LjTVttVXjt5YfvlRt/4rixufHhL22TGx+6Kv/8q7Yr/txnv+c3ufHzr3pfbvwfy0fmn2h8cWd+67/kz4Q0s4HNQ3xmZlZKTlBmZlZKTlBmZlZKTlBmZlZKDZ8kYWa18+Azy2g++/eNboYNMovOO6ZPzuselJmZlVLDe1CxYUOfX2Pl+9+aG1+zdX5+Hj/n+dz4hscXFl6jx5+ihwvYFi3kCrDlf92dGx9+yJtz420Th+fG2wvWwt3+ztWF157+4Hty4+OG59+REV//S+G5zMwquQdltonyaj318jxnSJraxfZX1Y8yG8ga3oMyGwC6VevJzHrGPSizTbcaOFDStpF5SdLlkuZKukPSBABJ90m6UNLdkr6YYjtLul3STcBhKdYk6Y/p+FmSXtO4j2bWOE5QZptuOvAYWa2neZL2BD4eEZOAC4BPpP1GA+cDbwc+lGJfBM6JiKOANQAR0QEcm46/EfhgVxeXNE1Sq6TWDauW1faTmTWQh/jMNlFErAfOBc5NhQXPBZ6X9EZgC+ChtOvSiHgCQNKaFHstMD+9vjdtGwn8TNJOwNZkpd67uv5FwEUAw7ffo2dlos1KrLwJqmA2W9Giqc99eN/CU42/N38G3Jb/9WBuvO/nFYKGDcvfUDCrscvZjgX3aujs+fnxPfNLxLePyJ9Z2HT7nwovnT8fcHCRtAuwOCLWkdV6Gg2sjYh3SXo/0DnVMe8HtQCYCPyRbHXzmcARwN8j4lRJnwOKVwo2G8DKm6DM+o99gRmpVyTgTOBCSbOARzdy7HeAyyV9Hng5xe4CvixpIvA88GTfNNus3JygzDZRQa2ng3L2y6v/9CTwzpzTvuqP2CqPNxsMPEnCzMxKyT0oswFk3x1H0dpH66KZ1Zt7UGZmVkqN70EVzEArKgXf9oF9cuPb/fL+wkt0rMqvZV50jcIZcwVtBdDw/PlssW5dfnzt2sJz5V+guOx6T222Ij/e0Zt/DQXt0pAhufHe3NuerltoZgODe1BmZlZKTlBmZlZKTlBmZlZKTlBmZlZKTlBmdSLp7alm1FxJsyV16w9vJY2W9IG+bp9Z2XQ9b6uWs6d6eK41h0/MjRetF1c0Uw+K1+/rqkptTxXN1itac69o/8J7W8MZa0Wz9VZv14trFLQr2tt7fq4eXqM/kbQ18BPgyIhYLGkUsHs3Dx8NfAC4qo+aZ1ZK7kGZ1ccxwLURsRggIpYBCyRdn3pUV0oaJmlbSbemGlHXSBoCnA5MSr2vvRr5IczqyQnKrD52AJ6tik0Dbkx1nx4GTgKWAlMi4iDgGeAQsp7X3IiYHBGPVJ+4sh5UW1tbn34Is3pygjKrj2eBHatiryXVgErf9wC2Aa6RNBc4miyxdSkiLoqIlohoGTduXA2bbNZYTlBm9fF74DhJ2wOkMu4Lgbek7QcAjwOnAL9Lvao/kJXvWA/kL81hNoA5QZnVQUQsIXuWdEXqHV0L3A0ck97vC1wJ3AL8q6TrgM7u0GJgi/RMao+6N96sQbqexVfL2VM9PNeyXTfLjY95fH2PL13L2XqFimaz9XTNvTqIgl9LmtbVbr0/e7WIuBOYXBWeW/X+frJkVe3IPmiSWam5B2VmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXU5TTzIaNH5W/YrGAB1JUrC89VVOq7aBr2a57IX2x0xMOLc+NdLU1atFhskbpMS6+hphEjcuMalj9Vf8QL+T+LNWPz/zks/+CBhdcetjz/XFs8mX8PN2w1vPBcRdZuk3/MyIee6/G5zKz/cA/KrA9IapbUlspq3CbpAkn5v0mYWS4nKLO+MzciDgEmAauAb3ZukOT/9sw2ouuVJMxsk0VESDoHeFDSAcA9wERJ7wF+TrYg7ArgNLLljaYDa4G/RsQnJP2SbGHZDcDUiFjUgI9hVndOUGZ1EBHrJHU+vJ0ZEf9T0hnA7Ii4WNIHycpvLAV+HRE/ltQkaTNgT+AdKdG9quclaVo6lgkTJtTnA5nVgYcZzOpA0nCyXhG8UmJjL+B0SXOAM4GxZFVzd5V0GXBaRKwHfgRMl/QD4FXPsVxuwwaqLntQz3x079z4sENfyI0vfbS4gvXQlfm5cPiS/P03X5K/+Gr7gdUldVKb9t6+8NpD1hTMIGzKXxy1fUR+ZYOV2xZXPFg/Mv9cK3bpyI1r2/zZiwftviA3/vTK0YXX3nNU/my25s3z44eNnJkbH92UPxdywtCRhdde1rE6N/6XdfkzPdcVVI0YRv7PCGBUU/69+vijpxUeU0JfIlvB/ECg8x/Fo8C8iJgOkHpLQyPiC+n9wylRXRURl0n6MvA+4NJ6N96sETzEZ9Z3Jkm6layW093A14CbKrZfBFwk6aPp/QXAyDT0BzAT2Aq4TlIAAZxal5ablYATlFkfSBMZ8sbbJlfsswb4cM4+M6reT6pZw8z6ET+DMjOzUnKCMjOzUnKCMjOzUuryGdR2378zNz7k1/lTWbea+JrCc728S/66cKu2y5/9tmZswYma8nPqujFdlJTfrGD23dD8GXZDhuWXld+wrnimWdPS/M83ZG3+59PCzXPj97XmVfuGkU8XX/uxJ0bnxv+2eGlu/I8j9s+Nt2+9ZW58zfji9fPWj8j/eawemx9ftnf+TEFtUbyaYnTk38PmGQW/Xx1eeCoz60fcgzIzs1JygjIzs1JygjIzs1JygjIzs1JygjLrgYo6T3Mk3ZNWJ8/brzV9/4akd9e3lWYDQ69WktjQ1pYb3+zm/DjANj2M26brqspwnvy5crBFF8cUbSuaz7lt95tTZnMj4gRJbwX+N3WcNyipKSLyp5+aDTDuQZn13v3AzpK+CyBpH0mXFO2cqurekarsNks6QdIX07aRkman11Ml3S7pTkmHpNgcSd8hW5/PbFBwgjLrvUnAdt3ZUVILsGNEvBP4OtnCsb8Hjk67HAtcL2kb4CTgXcCUtF+nmRExJefc0yS1SmptKxjdMOuPnKDMem5SRQ2n91bEi0ZJIauI21kH6l5gj4hYDTwp6XXACWS1oHYH9gZuJUtglX8Vfy85XA/KBiqvZm7Wc3Mj4gQASW8EdkrxN3VxzAJeSWYHAI+n1zPIquGOiIhnJa0D/gy8O1XQrVyixM+ebFBxgjLbNA8CIyTNAh4q2ikiWiUtlnQH2fyVzhpQM4GLSUN5EfGCpCuBuZI2pPOf2ZcfwKysFFG8ht2UphO7WODOrJxmdVzd1VDbgNbS0hKtra2NboZZj0iaHxEt1XE/gzIzs1JygjIzs1JygjIzs1JygjIzs1JygjIzs1JygjIzs1JygjIzs1JygjIzs1LyShJmvSBpC+Cm9PbNwPz0+n0RsaQxrTIbWJygzHohLfQ6GbLihBExuXNbX9Zscj0oG0w8xGdWA6ly7iWSbgTeWF37Ke3TWrF/Z8Xdc1Ldp1slHajMD9P7P0raKe33iKRfAt9rwMczawj3oMxq56mImFpZ+0nSQWQLwX6s4JjDgXdERLukJuAYYGlEHJwq9p4NnEG2Yvo7ImJp9QkkTSNbEZ0JEybU/lOZNYh7UGa101mv6VW1n3L27VzQ9uvAxZJ+BowH9gKOT/WmvgOMTvstyEtO4HpQNnC5B2VWO53PhopqP20uaQiwIzAmxeZGxB8knULWC7ofuCoizgGoqAfl50426DhBmdVYF7WfLgPmAbcBL6XYtZKGk/23eDpZTalDJN0KRDrmF3VsvllpOEGZbaK8OjYR8dmc2H8A/1EVOyLnlP/WnWuYDXR+BmVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVGVt9J0pz0tbzi9dYF+18iaZ+q2H6STs/Zdz9Jb6mK3SCpWdLhtf0kZgOHV5Iwo+v6Tj04x/1ka+n9f2mF8v2AkcA9KbYv8CDQTLaa+c29bLbZgOYEZbYRqRf1G7K18V6OiOPSpjMk7Q6sBI4HJgHvjojPS7oPuB0YS7a6+daSjo2Iw4GjgD8AnwbenspzvA+YCpwIbADOjIj70nlagX2B30TE+XX50GYl4CE+s42bCNwTEQeTJaJOd0bEFGAtWQKpNAb4YUScCvwE+EFKTgBvA+5M8RmptzaMbAX0dwCnAd+uOM8FKf4eSeOrGydpmqRWSa1tbW2b+lnNSsMJyiyHpEPSM6jLgLnAyvT6rIrd/pS+P8Ur5TM6LY2IBTnnHQmsiYj2qk3NwAMR0RERi3ilDtSKiHgslXl/ANi1+pyuB2UDlYf4zHJExGxgNmQTKCLim+n1zZKu6tyt4hBVnaKyftN6YEh6fShwS058EbBfemY1gVfKcYyUtAdZjak3pv3MBgX3oMw27gBJt0uaC7QBT/fw+HnAiZIuB44ke/4E2USJN0u6GlgDXEc29Hc5Wal3gKVk5TfmATdGxPOb8kHM+hNFROHGKU0nFm80K6lZHVdX92ZKQ9IpEXF5D/Zv7UktqJaWlmhtbe1d48waRNL8vH/n7kGZ1VFPkpPZYOcEZVZirqRrg5kTlJmZlZITlJmZlZITlJmZlZITlJmZlZITlJmZlZITlJmZlZITlJmZlZLX4jMbQObPn79C0mONbkcXxgIvNLoRBdy23tvU9u2SF3SCMhtYHivzH/f2dOmmenLbeq+v2tdlgirzmmZmZjaw+RmUmZmVkhOU2cByUaMbsBFlbp/b1nt90r4uy22YmZk1intQZmZWSk5QZv2EpCMlPSZpgaSzc7YPlzQjbb9bUnPFti+l+GOSjmhA286S9IikP0u6RdIuFds2SLo/fV1f67Z1s31TJbVVtOPjFds+Iunx9PWRBrTt+xXt+quklyq29em9k3SxpH9IeqhguyT9Z2r7nyXtX7Ft0+9bRPjLX/4q+RcwBPgbsBswDHgA2Ktqn08BP02vTwJmpNd7pf2HA7um8wypc9sOBkak16d3ti29X1GCezcVuDDn2K2Bhen7mPR6TD3bVrX/Z4CL63jv3gXsDzxUsP1o4CZAwIHA3bW8b+5BmfUPbwEWRMTCiFgHXAkcV7XPccCv0utrgEMlKcWvjIi1EfF3YEE6X93aFhG3RsSq9PYuYKcaXn+T29eFI4BZEbEkIpYCs4AjG9i2k4Eranj9LkXEbcCSLnY5Drg0MncBoyVtT43umxOUWf+wI/BUxfunUyx3n4hoB5YB23Tz2L5uW6V/Ifutu9Pmklol3SXpvTVsV0/b9/40THWNpJ17eGxft400LLorMLsi3Nf3bmOK2l+T++aVJMysbiSdBrQAkyrCu0TEM5J2A2ZLejAi/lbnpt0AXBERayV9gqwnekid27AxJwHXRMSGilgZ7l2fcQ/KrH94Bti54v1OKZa7j6ShwCjgxW4e29dtQ9JhwP8Cjo2ItZ3xiHgmfV8IzAEm1rBt3WpfRLxY0aafA2/u7rF93bYKJ1E1vFeHe7cxRe2vzX3rywds/vKXv2rzRTbasZBsiKfzYfreVft8mn+eJHFVer03/zxJYiG1nSTRnbZNJJsMsEdVfAwwPL0eCzxOF5ME+rB921e8Ph64K73eGvh7aueY9HrrerYt7fd6YBHpb1frde/SuZspniRxDP88SeKeWt43D/GZ9QMR0S7pDGAm2cyviyPiYUnfAloj4nrgF8B0SQvIHmyflI59WNJVwCNAO/Dp+Odhonq07XxgJHB1Nm+DJyPiWOANwM8kdZCN6JwXEY/Uqm09aN+Zko4luz9LyGb1ERFLJJ0D3JtO962I6GrSQF+0DbKf5ZWR/u+f9Pm9k3QFMBkYK+lp4OvAZqntPwVuJJvJtwBYBXw0bavJffNKEmZmVkp+BmVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqXkBGVmZqX036ZdM+hT7pbIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando o módulo helper para ajudar a plotar\n",
    "import helper\n",
    "\n",
    "# Testando nossa rede\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convertendo imagens 2D em tensores 1D\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Computando o log das probabilidades para cada imagem (com log_softmax)\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "\n",
    "# computa as probabilidades fazendo o exponencial do log das probabilidades \n",
    "ps = torch.exp(output)\n",
    "\n",
    "# plotando as probabilidades\n",
    "helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando e carregando modelos\n",
    "\n",
    "Agora aprenderemos a salvar e a carregar os modelos treinados. É muito importante pois, frequentemente, precisaremos salvar os modelos após treiná-los para usá-lo em predições sobre novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformação para normalizar os dados\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# download e carrega os dados de treinamento\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# download e carrega os dados de validação/teste\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando uma das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando a rede\n",
    "\n",
    "Para não ficar muito confuso com muito código, a parte de modelagem da rede que já apredemos foi escrita no arquivo `fc_model`. Com esse arquivo, podemos criar uma rede chamando `fc_model.Network` e treiná-la com o comando `fc_model.train`. Após treiná-lo, será usado como exemplo para demonstrar como salvar e carregar modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a rede, definindo o criterion (função loss) e o otimizador\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.731..  Test Loss: 0.984..  Test Accuracy: 0.637\n",
      "Epoch: 1/2..  Training Loss: 1.040..  Test Loss: 0.769..  Test Accuracy: 0.732\n",
      "Epoch: 1/2..  Training Loss: 0.871..  Test Loss: 0.685..  Test Accuracy: 0.738\n",
      "Epoch: 1/2..  Training Loss: 0.803..  Test Loss: 0.678..  Test Accuracy: 0.726\n",
      "Epoch: 1/2..  Training Loss: 0.761..  Test Loss: 0.619..  Test Accuracy: 0.761\n",
      "Epoch: 1/2..  Training Loss: 0.728..  Test Loss: 0.612..  Test Accuracy: 0.771\n",
      "Epoch: 1/2..  Training Loss: 0.724..  Test Loss: 0.561..  Test Accuracy: 0.790\n",
      "Epoch: 1/2..  Training Loss: 0.688..  Test Loss: 0.570..  Test Accuracy: 0.792\n",
      "Epoch: 1/2..  Training Loss: 0.649..  Test Loss: 0.600..  Test Accuracy: 0.776\n",
      "Epoch: 1/2..  Training Loss: 0.664..  Test Loss: 0.561..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.608..  Test Loss: 0.538..  Test Accuracy: 0.800\n",
      "Epoch: 1/2..  Training Loss: 0.601..  Test Loss: 0.537..  Test Accuracy: 0.797\n",
      "Epoch: 1/2..  Training Loss: 0.638..  Test Loss: 0.521..  Test Accuracy: 0.803\n",
      "Epoch: 1/2..  Training Loss: 0.601..  Test Loss: 0.518..  Test Accuracy: 0.810\n",
      "Epoch: 1/2..  Training Loss: 0.587..  Test Loss: 0.523..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.563..  Test Loss: 0.523..  Test Accuracy: 0.801\n",
      "Epoch: 1/2..  Training Loss: 0.569..  Test Loss: 0.516..  Test Accuracy: 0.816\n",
      "Epoch: 1/2..  Training Loss: 0.590..  Test Loss: 0.491..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.622..  Test Loss: 0.523..  Test Accuracy: 0.803\n",
      "Epoch: 1/2..  Training Loss: 0.624..  Test Loss: 0.514..  Test Accuracy: 0.809\n",
      "Epoch: 1/2..  Training Loss: 0.591..  Test Loss: 0.493..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.597..  Test Loss: 0.484..  Test Accuracy: 0.824\n",
      "Epoch: 1/2..  Training Loss: 0.594..  Test Loss: 0.477..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.570..  Test Loss: 0.476..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.486..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.604..  Test Loss: 0.480..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.472..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.560..  Test Loss: 0.489..  Test Accuracy: 0.817\n",
      "Epoch: 2/2..  Training Loss: 0.518..  Test Loss: 0.466..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.476..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.526..  Test Loss: 0.460..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.523..  Test Loss: 0.454..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.521..  Test Loss: 0.452..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.449..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.535..  Test Loss: 0.471..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.491..  Test Loss: 0.455..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.464..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.499..  Test Loss: 0.468..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.458..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.447..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.557..  Test Loss: 0.441..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.447..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.516..  Test Loss: 0.460..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.443..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.546..  Test Loss: 0.451..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.502..  Test Loss: 0.448..  Test Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2) # print every 40 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando e carregando o modelo\n",
    "\n",
    "Como já devem ter percebido, não é prático ficar treinando a rede toda hora que precisar usá-la. Em vez disso, podemos salvar o modelo treinado e carregá-lo para usar mais tarde para continuar o treinamento ou fazer predições.\n",
    "\n",
    "Os parâmetros da rede são armazenados em um `state_dict`. Esse 'dicionário de estados' contém os pesos e biases para cada uma das camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nosso modelo: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "As chaves do dicionário: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Nosso modelo: \\n\\n\", model, '\\n')\n",
    "print(\"As chaves do dicionário: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modo mais simples de salvar o modelo é usando `torch.save`. Podemos, por exemplo, salvá-lo num arquivo `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E depois carregá-lo usando `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sequência, carregamos nosso state_dict na rede, usando o comando `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece bem simples, mas como já estamos acostumados, nunca é tão fácil. Carregar o dicionário de estados só funciona se a arquitetura for exatamente a mesma que a do modelo salvo no checkpoint. Se criar um novo modelo com alguma diferença na arquitetura o método falha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-eda97a2d2d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Vai dar erro porque o número de neurônios nas camadas escondidas são diferentes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Testando uma nova rede\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# Vai dar erro porque o número de neurônios nas camadas escondidas são diferentes\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que precisaremos reconstruir exatamente o mesmo modelo usado no treinamento, salvando também as informações sobre a arquitetura do modelo no checkpoint, junto com o state dict. Para isso, construiremos um dicionário com todas as informações necessárias reconstruir o modelo completamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora o checkpoint tem todas as informações necessárias para reconstruir o modelo treinado. Podemos criar uma função para fazer o processo, assim como podemos, de forma similar, criar uma função para carregar os checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
